<!doctype html>
<html lang="en" data-bs-theme="light">

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-L8QGKHMHLL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-L8QGKHMHLL');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Abhay Deshpande</title>
    <link rel="icon" href="img/favicon-32x32.png">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="drone2d/drone2d.css">

    <script src="drone2d/deps/js-yaml.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script src="drone2d/dynamics.js" type="module"></script>
    <script src="drone2d/drone2d.js" type="module"></script>
</head>

<body>
    <nav class="navbar navbar-expand-lg bg-body-tertiary mb-3">
        <div class="container">
            <a class="navbar-brand" href="/index.html">Abhay Deshpande</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                    <li class="nav-item">
                        <a class="nav-link active" aria-current="page" href="index.html">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="index.html#publications">Publications</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="projects.html">Projects</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="resume.html">Resume</a>
                    </li>
                </ul>
                <ul class="navbar-nav me-2 ms-auto mb-2 mb-lg-0">
                    <li class="nav-item dropdown">
                        <button
                            class="btn btn-link nav-link py-2 px-0 px-lg-2 dropdown-toggle d-flex align-items-center"
                            id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown"
                            data-bs-display="static" aria-label="Toggle theme (light)">
                            <span class="theme-icon-active">
                                <i class="bi bi-sun-fill"></i>
                            </span>
                            <span class="d-lg-none ms-2" id="bd-theme-text">Toggle theme</span>
                        </button>
                        <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
                            <li>
                                <button type="button" class="dropdown-item d-flex align-items-center active"
                                    data-bs-theme-value="light" aria-pressed="true">
                                    <i class="bi bi-sun-fill me-2"></i>
                                    Light
                                </button>
                            </li>
                            <li>
                                <button type="button" class="dropdown-item d-flex align-items-center"
                                    data-bs-theme-value="dark" aria-pressed="false">
                                    <i class="bi bi-moon-stars-fill me-2"></i>
                                    Dark
                                </button>
                            </li>
                            <li>
                                <button type="button" class="dropdown-item d-flex align-items-center"
                                    data-bs-theme-value="auto" aria-pressed="false">
                                    <i class="bi bi-circle-half me-2"></i>
                                    Auto
                                </button>
                            </li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="container mt-5">
        <div class="row justify-content-center align-items-center g-3">
            <!-- Bio displayed on lg and larger -->
            <div class="col-lg-3 order-lg-last text-center bio d-none d-lg-block">
                <img src="img/me.jpg" alt="Photo of Abhay Deshpande" />
                <div class="row mt-3 justify-content-center">
                    <div class="col-auto">
                        <a href="mailto:a.deshpande012@gmail.com" target="_blank">
                            <i class="icon bi bi-envelope-fill"></i>
                        </a>
                    </div>
                    <div class="col-auto">
                        <a href="https://github.com/abhaybd" target="_blank">
                            <i class="icon bi bi-github"></i>
                        </a>
                    </div>
                    <div class="col-auto">
                        <a href="https://scholar.google.com/citations?user=KT2f52IAAAAJ" target="_blank">
                            <i class="icon ai ai-google-scholar"></i>
                        </a>
                    </div>
                    <div class="col-auto">
                        <a href="https://www.semanticscholar.org/author/Abhay-Deshpande/40755038" target="_blank">
                            <i class="icon ai ai-semantic-scholar"></i>
                        </a>
                    </div>
                    <div class="col-auto">
                        <a href="https://www.linkedin.com/in/abhaybd/" target="_blank">
                            <i class="icon bi bi-linkedin"></i>
                        </a>
                    </div>
                </div>
            </div>
            <!-- Bio displayed on smaller than lg -->
            <div class="col-lg-3 order-lg-last text-center bio d-lg-none">
                <div class="row">
                    <div class="col d-flex align-items-center">
                        <img src="img/me.jpg" alt="Photo of Abhay Deshpande" />
                    </div>
                    <div class="col-auto justify-content-center d-flex flex-column gap-2">
                        <div class="row">
                            <div class="col-auto">
                                <a href="mailto:a.deshpande012@gmail.com" target="_blank">
                                    <i class="icon bi bi-envelope-fill"></i>
                                    <span class="ms-2">a.deshpande012@gmail.com</span>
                                </a>
                            </div>
                        </div>
                        <div class="row">
                            <div class="col-auto">
                                <a href="https://github.com/abhaybd" target="_blank">
                                    <i class="icon bi bi-github"></i>
                                    <span class="ms-2">abhaybd</span>
                                </a>
                            </div>
                        </div>
                        <div class="row">
                            <div class="col-auto">
                                <a href="https://scholar.google.com/citations?user=KT2f52IAAAAJ" target="_blank">
                                    <i class="icon ai ai-google-scholar" style="width: 1em;"></i>
                                    <span class="ms-2">Google Scholar</span>
                                </a>
                            </div>
                        </div>
                        <div class="row">
                            <div class="col-auto">
                                <a href="https://www.semanticscholar.org/author/Abhay-Deshpande/40755038" target="_blank">
                                    <i class="icon ai ai-semantic-scholar"></i>
                                    <span class="ms-2">Semantic Scholar</span>
                                </a>
                            </div>
                        </div>
                        <div class="row">
                            <div class="col-auto">
                                <a href="https://www.linkedin.com/in/abhaybd/" target="_blank">
                                    <i class="icon bi bi-linkedin"></i>
                                    <span class="ms-2">/in/abhaybd</span>
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="col-lg-6 order-lg-first px-3 px-sm-2">
                <p>
                    Hey! I'm Abhay Deshpande, a Predoctoral Researcher (PYI) at <a
                        href="https://prior.allenai.org/">PRIOR @ AI2</a>.
                    Previously, I completed my undergrad in CS and Math at the University of Washington, where I worked
                    with <a href="https://www.cs.washington.edu/people/faculty/siddh">Prof. Siddartha Srinivasa</a>
                    in the <a href="https://personalrobotics.cs.washington.edu/">Personal Robotics
                        Laboratory</a>,
                    and <a href="https://homes.cs.washington.edu/~abhgupta/">Prof. Abhishek Gupta</a>
                    in the <a href="https://weirdlab.cs.washington.edu/">WEIRDLab</a>.
                    My research primarily focuses on <b>learning for robotic manipulation</b>.
                </p>
                <p>
                    Through research, I seek to build generalist robotic systems that can learn about the world through
                    data, and operate effectively and intelligently in everyday circumstances.
                </p>
                <p>
                    <b>I am applying to PhD programs in <u>robot learning</u> starting in Fall 2026!</b>
                </p>
            </div>
        </div>
    </div>

    <div id="publications" class="container mt-5">
        <div class="row justify-content-center">
            <div class="col-lg-9">
                <h1>Publications</h1>
                <h2 class="mt-4">Conference Papers</h2>
                <div class="card flex-row publication">
                    <div class="card-header border-0 d-sm-block d-none">
                        <img src="img/graspmolmo.png" alt="Image of a bimanual robot opening a water bottle.">
                    </div>
                    <div class="card-block px-2">
                        <h4 class="card-title">GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation</h4>
                        <h6 class="card-subtitle mb-2 text-muted">
                            <b>Abhay Deshpande</b>, Yuquan Deng, Arijit Ray, Jordi Salvador, Winson Han, Jiafei Duan, Kuo-Hao Zeng, Yuke Zhu, Ranjay Krishna, Rose Hendrix
                        </h6>
                        <p class="card-text"><em>CoRL 2025</em></p>
                        <a class="card-link" data-bs-toggle="collapse" href="#graspmolmo_abstract" aria-expanded="false"
                            aria-controls="graspmolmo_abstract">
                            Abstract
                        </a>
                        <a href="https://arxiv.org/abs/2505.13441" class="card-link">arXiv</a>
                        <a href="https://abhaybd.github.io/GraspMolmo/" class="card-link">Project Website</a>
                        <div class="collapse mt-1 mb-2" id="graspmolmo_abstract">
                            <div class="card card-body pt-2 pb-2">
                                <p>
                                    We present GraspMolmo, a generalizable open-vocabulary task-oriented grasping (TOG) model, and
                                    PRISM, a large-scale synthetic dataset used to train it. GraspMolmo predicts semantically
                                    appropriate, stable grasps conditioned on a natural language instruction and a single RGB-D
                                    frame. For instance, given "pour me some tea", GraspMolmo selects a grasp on a teapot handle
                                    rather than its body or lid.
                                    Unlike prior TOG methods, which are limited by small datasets, simplistic language, and
                                    uncluttered scenes, GraspMolmo learns from PRISM, a novel large-scale synthetic dataset of 379k
                                    samples featuring cluttered environments and diverse, realistic task descriptions. We fine-tune
                                    the Molmo visual-language model on this data, enabling GraspMolmo to generalize to novel
                                    open-vocabulary instructions and objects.
                                    In challenging real-world evaluations, GraspMolmo achieves state-of-the-art results, with a 70%
                                    prediction success on complex tasks, compared to the 35% achieved by the next best alternative.
                                    GraspMolmo also successfully demonstrates the ability to predict semantically correct bimanual
                                    grasps zero-shot.
                                    We release our synthetic dataset, code, model, and benchmarks to accelerate research in
                                    task-semantic robotic manipulation.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="card flex-row publication mt-3">
                    <div class="card-header border-0 d-sm-block d-none">
                        <img src="img/ccil_ext.png" alt="A collage of multiple images of a robot using chopsticks to accomplish different tasks.">
                    </div>
                    <div class="card-block px-2">
                        <h4 class="card-title">Data Efficient Behavior Cloning for Fine Manipulation via Continuity-based Corrective Labels</h4>
                        <h6 class="card-subtitle mb-2 text-muted">
                            <b>Abhay Deshpande</b>, Liyiming Ke, Quinn Pfeifer, Abhishek Gupta, Siddhartha Srinivasa
                        </h6>
                        <p class="card-text"><em>IROS 2024 <span class="text-danger">(Oral Presentation)</span></em></p>
                        <a class="card-link" data-bs-toggle="collapse" href="#ccil_ext_abstract" aria-expanded="false"
                            aria-controls="ccil_ext_abstract">
                            Abstract
                        </a>
                        <a href="https://arxiv.org/abs/2405.19307" class="card-link">arXiv</a>
                        <a href="https://personalrobotics.github.io/CCIL/" class="card-link">Project Website</a>
                        <div class="collapse mt-1 mb-2" id="ccil_ext_abstract">
                            <div class="card card-body pt-2 pb-2">
                                <p>
                                    We consider imitation learning with access only to expert demonstrations, whose real-world
                                    application is often limited by covariate shift due to compounding errors during execution. We
                                    investigate the effectiveness of the Continuity-based Corrective Labels for Imitation Learning
                                    (CCIL) framework in mitigating this issue for real-world fine manipulation tasks. CCIL generates
                                    corrective labels by learning a locally continuous dynamics model from demonstrations to guide the
                                    agent back toward expert states. Through extensive experiments on peg insertion and fine grasping,
                                    we provide the first empirical validation that CCIL can significantly improve imitation learning
                                    performance despite discontinuities present in contact-rich manipulation. We find that: (1)
                                    real-world manipulation exhibits sufficient local smoothness to apply CCIL, (2) generated corrective
                                    labels are most beneficial in low-data regimes, and (3) label filtering based on estimated dynamics
                                    model error enables performance gains. To effectively apply CCIL to robotic domains, we offer a
                                    practical instantiation of the framework and insights into design choices and hyperparameter
                                    selection. Our work demonstrates CCIL's practicality for alleviating compounding errors in imitation
                                    learning on physical robots.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="card flex-row publication mt-3">
                    <div class="card-header border-0 d-sm-block d-none">
                        <img src="img/ccil.png" alt="Image of a diagram of the label generation technique in CCIL.">
                    </div>
                    <div class="card-block px-2">
                        <h4 class="card-title">CCIL: Continuity-based Data Augmentation for Corrective Imitation Learning</h4>
                        <h6 class="card-subtitle mb-2 text-muted">
                            Liyiming Ke*, Yunchu Zhang*, <b>Abhay Deshpande</b>, Siddhartha Srinivasa, Abhishek Gupta
                        </h6>
                        <p class="card-text"><em>ICLR 2024</em></p>
                        <a class="card-link" data-bs-toggle="collapse" href="#ccil_abstract" aria-expanded="false"
                            aria-controls="ccil_abstract">
                            Abstract
                        </a>
                        <a href="https://arxiv.org/abs/2310.12972" class="card-link">arXiv</a>
                        <a href="https://personalrobotics.github.io/CCIL/" class="card-link">Project Website</a>
                        <div class="collapse mt-1 mb-2" id="ccil_abstract">
                            <div class="card card-body pt-2 pb-2">
                                <p>
                                    We present a new technique to enhance the robustness of imitation learning methods by
                                    generating corrective data to account for compounding errors and disturbances. While
                                    existing methods rely on interactive expert labeling, additional offline datasets, or
                                    domain-specific invariances, our approach requires minimal additional assumptions beyond
                                    access to expert data. The key insight is to leverage local continuity in the environment
                                    dynamics to generate corrective labels. Our method first constructs a dynamics model from
                                    the expert demonstration, encouraging local Lipschitz continuity in the learned model. In
                                    locally continuous regions, this model allows us to generate corrective labels within the
                                    neighborhood of the demonstrations but beyond the actual set of states and actions in the
                                    dataset. Training on this augmented data enhances the agent's ability to recover from
                                    perturbations and deal with compounding errors. We demonstrate the effectiveness of our
                                    generated labels through experiments in a variety of robotics domains in simulation that
                                    have distinct forms of continuity and discontinuity, including classic control problems,
                                    drone flying, navigation with high-dimensional sensor observations, legged locomotion, and
                                    tabletop manipulation.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="card flex-row publication mt-3">
                    <div class="card-header border-0 d-sm-block d-none">
                        <video autoplay loop muted>
                            <source src="vid/cherrybot_human_ball.mp4" type="video/mp4">
                            Video of a robot grabbing a ball that is being shaken around by a hand.
                        </video>
                    </div>
                    <div class="card-block px-2">
                        <h4 class="card-title">Cherry Picking with Reinforcement Learning</h4>
                        <h6 class="card-subtitle mb-2 text-muted">
                            Yunchu Zhang*, Liyiming Ke*, <b>Abhay Deshpande</b>, Abhishek Gupta, Siddhartha Srinivasa
                        </h6>
                        <p class="card-text"><em>RSS 2023</em></p>
                        <a class="card-link" data-bs-toggle="collapse" href="#cherrybot_abstract" aria-expanded="false"
                            aria-controls="cherrybot_abstract">
                            Abstract
                        </a>
                        <a href="https://arxiv.org/abs/2303.05508" class="card-link">arXiv</a>
                        <a href="https://goodcherrybot.github.io/" class="card-link">Project Website</a>
                        <div class="collapse mt-1 mb-2" id="cherrybot_abstract">
                            <div class="card card-body pt-2 pb-2">
                                <p>
                                    Grasping small objects surrounded by unstable or non-rigid material plays a crucial role in
                                    applications such as surgery, harvesting, construction, disaster recovery, and assisted
                                    feeding. This task is especially difficult when fine manipulation is required in the
                                    presence of sensor noise and perception errors; errors inevitably trigger dynamic motion,
                                    which is challenging to model precisely. Circumventing the difficulty to build accurate
                                    models for contacts and dynamics, data-driven methods like reinforcement learning (RL) can
                                    optimize task performance via trial and error, reducing the need for accurate models of
                                    contacts and dynamics. Applying RL methods to real robots, however, has been hindered by
                                    factors such as prohibitively high sample complexity or the high training infrastructure
                                    cost for providing resets on hardware. This work presents CherryBot, an RL system that uses
                                    chopsticks for fine manipulation that surpasses human reactiveness for some dynamic grasping
                                    tasks. By integrating imprecise simulators, suboptimal demonstrations and external state
                                    estimation, we study how to make a real-world robot learning system sample efficient and
                                    general while reducing the human effort required for supervision. Our system shows continual
                                    improvement through 30 minutes of real-world interaction: through reactive retry, it
                                    achieves an almost 100% success rate on the demanding task of using chopsticks to grasp
                                    small objects swinging in the air. We demonstrate the reactiveness, robustness and
                                    generalizability of CherryBot to varying object shapes and dynamics (e.g., external
                                    disturbances like wind and human perturbations). Videos are available at
                                    <a href="https://goodcherrybot.github.io/">this URL</a>.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <h2 class="mt-4">Other</h2>
                <div class="card flex-row publication">
                    <div class="card-header border-0 d-sm-block d-none">
                        <img src="img/uw.png" alt="Logo of the University of Washington.">
                    </div>
                    <div class="card-block px-2">
                        <h4 class="card-title">Undergraduate Thesis</h4>
                        <a class="card-link" data-bs-toggle="collapse" href="#ugrad_thesis_abstract" aria-expanded="false"
                            aria-controls="ugrad_thesis_abstract">
                            Abstract
                        </a>
                        <a href="./pdf/ugrad_thesis.pdf" class="card-link">Document</a>
                        <div class="collapse mt-1 mb-2" id="ugrad_thesis_abstract">
                            <div class="card card-body pt-2 pb-2">
                                <p>
                                    We consider imitation learning with access only to expert demonstrations, whose real-world application is often limited
                                    by covariate shift due to compounding errors during execution. We investigate the effectiveness of the Continuity-based
                                    Corrective Labels for Imitation Learning (CCIL) framework in mitigating this issue for real-world fine manipulation
                                    tasks. CCIL generates corrective labels by learning a locally continuous dynamics model from demonstrations to guide the
                                    agent back toward expert states. Through extensive experiments on peg insertion and fine grasping, we provide the first
                                    empirical validation that CCIL can significantly improve imitation learning performance despite discontinuities present
                                    in contact-rich manipulation. We find that: (1) real-world manipulation exhibits sufficient local smoothness to apply
                                    CCIL, (2) generated corrective labels are most beneficial in low-data regimes, and (3) label filtering based on
                                    estimated dynamics model error enables performance gains. To effectively apply CCIL to robotic domains, we offer a
                                    practical instantiation of the framework and insights into design choices and hyperparameter selection. Our work
                                    demonstrates CCIL's practicality for alleviating compounding errors in imitation learning on physical robots.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <button class="btn btn-danger" id="launch-button">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-rocket-takeoff" viewBox="0 0 16 16" style="transform: translateY(-1px);">
            <path d="M9.752 6.193c.599.6 1.73.437 2.528-.362s.96-1.932.362-2.531c-.599-.6-1.73-.438-2.528.361-.798.8-.96 1.933-.362 2.532"/>
            <path d="M15.811 3.312c-.363 1.534-1.334 3.626-3.64 6.218l-.24 2.408a2.56 2.56 0 0 1-.732 1.526L8.817 15.85a.51.51 0 0 1-.867-.434l.27-1.899c.04-.28-.013-.593-.131-.956a9 9 0 0 0-.249-.657l-.082-.202c-.815-.197-1.578-.662-2.191-1.277-.614-.615-1.079-1.379-1.275-2.195l-.203-.083a10 10 0 0 0-.655-.248c-.363-.119-.675-.172-.955-.132l-1.896.27A.51.51 0 0 1 .15 7.17l2.382-2.386c.41-.41.947-.67 1.524-.734h.006l2.4-.238C9.005 1.55 11.087.582 12.623.208c.89-.217 1.59-.232 2.08-.188.244.023.435.06.57.093q.1.026.16.045c.184.06.279.13.351.295l.029.073a3.5 3.5 0 0 1 .157.721c.055.485.051 1.178-.159 2.065m-4.828 7.475.04-.04-.107 1.081a1.54 1.54 0 0 1-.44.913l-1.298 1.3.054-.38c.072-.506-.034-.993-.172-1.418a9 9 0 0 0-.164-.45c.738-.065 1.462-.38 2.087-1.006M5.205 5c-.625.626-.94 1.351-1.004 2.09a9 9 0 0 0-.45-.164c-.424-.138-.91-.244-1.416-.172l-.38.054 1.3-1.3c.245-.246.566-.401.91-.44l1.08-.107zm9.406-3.961c-.38-.034-.967-.027-1.746.163-1.558.38-3.917 1.496-6.937 4.521-.62.62-.799 1.34-.687 2.051.107.676.483 1.362 1.048 1.928.564.565 1.25.941 1.924 1.049.71.112 1.429-.067 2.048-.688 3.079-3.083 4.192-5.444 4.556-6.987.183-.771.18-1.345.138-1.713a3 3 0 0 0-.045-.283 3 3 0 0 0-.3-.041Z"/>
            <path d="M7.009 12.139a7.6 7.6 0 0 1-1.804-1.352A7.6 7.6 0 0 1 3.794 8.86c-1.102.992-1.965 5.054-1.839 5.18.125.126 3.936-.896 5.054-1.902Z"/>
        </svg>
        <span style="margin-left: 4px;">Launch!</span>
    </button>

    <script src="js/dark_mode.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL"
        crossorigin="anonymous"></script>
</body>

</html>